\chapter{Background}

\section{Infortuni, Sicurezza Industriale e DPI}

% prima della parte normativa, avvalorare l'introduzione precedente con dati reali

In questa sezione si vedranno delle statistiche relative agli infortuni sul lavoro, quale sia la risposta normativa al problema della sicurezza industriale dal punto di vista degli attori coinvolti, integrata con la definizione di dispositivi di sicurezza. Questo tema è di primaria importanza per garantire non solo la salute e il benessere dei lavoratori, ma anche l'efficienza operativa e la sostenibilità economica delle aziende. Secondo i dati forniti dall'Istituto Nazionale per l'Assicurazione contro gli Infortuni sul Lavoro (INAIL), nel 2022 il settore manifatturiero ha registrato un tasso di infortuni del 13,9\% sul totale\cite{a1inail2023}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/totaleinfortuni.png}
    \caption{Infortuni sul lavoro accertati positivi per genere
e modalità di accadimento nell'anno 2022.}
    \label{fig:infortot}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/infortuni_industria_e_servizi.png}
    \caption{infortuni in occasione di lavoro accertati positivi per settore di attività nell'anno 2022}
    \label{fig:inforsplit}
\end{figure}

\noindent Essi comportano gravi conseguenze per i dipendenti, inclusi infortuni permanenti, invalidità e, nei casi più gravi, decessi. Oltre al costo umano, gli incidenti sul lavoro hanno un impatto significativo sull'economia delle aziende, generando costi diretti come spese mediche e indennità di infortunio, e costi indiretti come perdita di produttività, danni reputazionali e aumento dei premi assicurativi. L'EU-Occupational Safety and Health Administration (EU-OSHA) a questo proposito ha stimato in due diversi approcci l'impatto degli incidenti sul lavoro all'interno dell'Unione Europea\cite{a2osha-eu}. Nell'indagine sono stati presi in esame i dati relativi a 5 Paesi, poiché più completi e accessibili (tra cui figura anche l'Italia) e sono stati mostrati i risultati seguendo due diversi approcci: uno bottom-up, perché prende i valori dei costi per ciascun infortunio e li valuta globalmente; l'altro top-down, in quanto stima l'impatto dell'infortunio sulla vita del lavoratore e da valori macroeconomici come il PIL pro-capite valuta il costo effettivo dell'infortunio sul singolo. In termini pratici, nel primo caso si tiene conto dei costi diretti, indiretti e immateriali (effetti sulla vita e sulla salute) mentre nel secondo del valore monetario espresso in DALY, cioè il costo in termini di anni di vita persi a causa di un infortunio o di una malattia.

\vspace{0,5cm}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/onere_infortuni_ba.png}
    \caption{onere economico complessivo stimato (approccio bottom up)}
    \label{fig:osha_table1}
\end{figure}
\vspace{0,5cm} 

\noindent Il risultato di queste analisi ha mostrato che per l'Italia il costo di un infortunio o malattia causata dal posto di lavoro aveva un impatto percentuale sul PIL del 6,3\% nel primo caso, mentre nell'approccio top down, riferendosi alla metodologia VSLY - considerata più coerente con i risultati dell'approccio bottom up - il valore mediano era del 7,7\% rispetto alla produzione interna. I valori ottenuti, indipendentemente dal criterio utilizzato, non si discostano troppo l'uno dall'altro, confermando l'attendibilità dell'analisi. Si può dedurre perciò quanto questo problema sia concreto e impatti sulla società e sull'economia dell'Italia, dove il posto di lavoro è in gran parte costituito dall'industria. 
 
\renewcommand{\floatpagefraction}{0.7} % Richiede che questa figura occupi almeno il 70% della pagina 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/onere_infortuni_td.png}
    \caption{stima dei costi complessivi approccio top down}
    \label{fig:osha_table2}
\end{figure} 

%flusso logico: 
%	1)definizione dpi in ambito legislativo
%	2)obblighi datore di lavoro 
%	3)classificazione dpi 
%	4)regolamentazione europea su produzione dpi
 
\noindent L'utilizzo corretto dei Dispositivi di Protezione Individuale (DPI) è fondamentale per prevenire tali incidenti. Secondo la legislazione italiana, per DPI si intende \emph{qualsiasi attrezzatura destinata ad essere indossata e tenuta dal lavoratore allo scopo di proteggerlo contro uno o più rischi suscettibili di minacciarne la sicurezza o la salute durante il lavoro, nonché ogni complemento o accessorio destinato a tale scopo}\cite{a3decreto81}.

\medskip

\noindent La normativa in materia di sicurezza sul lavoro è un sistema complesso e articolato, volto a tutelare la salute e l'incolumità dei lavoratori in ogni settore produttivo. Il fulcro di questo sistema è rappresentato dal \textbf{Decreto Legislativo 81/2008}, conosciuto come \emph{Testo Unico sulla Salute e Sicurezza sul Lavoro}. Questo decreto introduce una serie di obblighi inderogabili per i datori di lavoro, al fine di garantire un ambiente salubre e sicuro. Tra i principi cardine si evidenziano:

\begin{itemize}
	%articoli 17-28-29
    \item \textbf{Valutazione dei rischi}: il datore di lavoro, con l'ausilio di un responsabile di sicurezza ed un medico esperto, è tenuto ad effettuare un'attenta e completa valutazione di tutte le criticità presenti sul luogo di lavoro. A questo scopo deve redarre un documento dove vengono presi in considerazione tutti i criteri utilizzati nella valutazione dei rischi.
    %articoli 15-28
    \item \textbf{Programmazione della prevenzione}: sulla base dell'analisi, il datore di lavoro, nello stesso documento, deve individuare i dispositivi di sicurezza necessari nelle attività lavorative ed elaborare un piano di prevenzione, nell'ottica di eliminare o ridurre al minimo i rischi trovati. Questo piano deve essere integrato con le condizioni tecniche, ambientali e produttive dell'azienda, garantendo la sua effettiva applicabilità e sostenibilità.
    %articolo 36-37
    \item \textbf{Informazione e formazione dei lavoratori}: i lavoratori devono essere informati in modo chiaro e completo sui rischi generali dell'azienda e su quelli specifici a cui sono esposti durante lo svolgimento delle loro mansioni, su come effettuare un primo soccorso e a chi rivolgersi nell'ottica di prevenzione dei rischi. Devono inoltre ricevere una formazione adeguata sulla loro prevenzione, adottare comportamenti sicuri e utilizzare correttamente macchinari e dispositivi di protezione individuale. L'informazione e la formazione devono essere fornite prima dell'inizio dell'attività lavorativa e devono essere ripetute periodicamente, garantendo l'aggiornamento costante dei lavoratori, nel caso ad esempio vengano cambiate le mansioni, oppure siano introdotte nuove attrezzature e tecnologie.
    %articolo 41
    \item \textbf{Sorveglianza sanitaria}: questa misura è fondamentale per monitorare lo stato di salute degli operatori in relazione ai rischi cui sono esposti, prevenire l'insorgenza di malattie professionali e garantire l'idoneità alla mansione. La sorveglianza sanitaria è effettuata da un medico competente, che ha il compito di visitare i lavoratori, effettuare gli accertamenti sanitari necessari e rilasciare il giudizio di idoneità.
\end{itemize}




\noindent I DPI rappresentano l'ultima barriera di protezione, quando le misure tecniche e organizzative non sono sufficienti a eliminare o ridurre i rischi. Pertanto, la loro scelta, il loro utilizzo e la loro manutenzione devono essere effettuati con la massima attenzione e responsabilità. Vengono suddivisi nelle seguenti categorie in base alla loro funzione:

\begin{itemize}
    \item \textbf{Protezione della testa}: caschi di protezione per l'industria.
    
    \item \textbf{Protezione dell'udito}: cuffie antirumore, tappi auricolari.
    
    \item \textbf{Protezione degli occhi e del viso}: occhiali protettivi, visiere.
    
    \item \textbf{Protezione delle vie respiratorie}: mascherine antipolvere e respiratorie.
    
    \item \textbf{Protezione degli arti superiori e inferiori}: guanti di protezione, scarpe antinfortunistiche, ginocchiere.
    
    \item \textbf{Indumenti di protezione}: tute, grembiuli, giubbotti ad alta visibilità.
\end{itemize}

\noindent Gli standard, giocano un ruolo fondamentale nel definire tecnicamente i criteri di produzione, utilizzo e manutenzione dei DPI, garantendo un elevato livello di protezione per gli utenti. La legge europea, come evidenziato nel \textbf{Regolamento (UE) 2016/425} stabilisce i requisiti che i DPI devono soddisfare nel mercato unico\cite{a4regolamento425}, tra cui:

\begin{itemize}
	%gli allegati sono più coerenti con una documentazione tecnica come questa
	%allegato II 1.1.1 (correlato ad articolo 5)
    \item \textbf{Ergonomia}: i DPI devono essere progettati e fabbricati in modo da essere comodi da indossare e non limitare la libertà di movimento del lavoratore, evitando di interferire con lo svolgimento delle sue attività, garantendone allo stesso tempo la sicurezza. 
    
    %allegato II 1.1.2.2 (correlato ad articolo 5)
    \item \textbf{Livelli e classi di protezione}: i DPI devono fornire un livello di protezione adeguato al rischio specifico da cui proteggono. La classificazione dei DPI in base al livello di protezione consente di scegliere il dispositivo più idoneo in relazione al rischio da prevenire.
    
    %articolo 16-17
    \item \textbf{Marcatura}: i DPI devono essere marcati con il simbolo \textbf{CE}, a indicare la loro conformità ai requisiti di sicurezza dell'Unione Europea. La marcatura CE deve essere apposta in modo visibile, leggibile e indelebile sul DPI o sulla sua confezione.

    %allegato II 1.4 (correlato ad articolo 8)
    \item \textbf{Istruzioni e informazioni del fabbricante}: i DPI devono essere accompagnati da istruzioni chiare e complete (e.g. rischi coperti, prestazioni, classi di protezione, accessori, pezzi di ricambio etc.) per l'utilizzatore, che indichino in modo dettagliato come impiegare, conservare, pulire e manutenere correttamente il dispositivo. Le istruzioni devono essere redatte in una lingua comprensibile nello Stato membro in cui il DPI è commercializzato. I dispositivi fabbricati devono avere una sorgente (il produttore e il suo indirizzo) ed essere identificati dal lotto messo in commercio.
\end{itemize} 

% definizione computer vision
% storia delle reti neurali
% applicazioni di computer vision sul lavoro
\section{Computer Vision}

La computer vision è un campo dell'informatica incentrata sulla comprensione del contenuto di immagini o video per mezzo di un calcolatore. I task che si possono svolgere sono di diversi tipologie, tra cui la classificazione, l'object detection, la segmentazione, il riconoscimento di volti, l'encoding e l'applicazione di filtri per la modifica delle immagini originali. La ricerca sulle reti neurali nell'ambito della computer vision è stata tra le prime a mostrare le potenzialità di questa tecnologia nella risoluzione di problemi nel mondo reale. Storicamente l'insieme di diversi sviluppi nelle discipline di neuroscienza, deep learning e matematica ha permesso il raggiungimento di questo traguardo. Le scoperte relative al neurone biologico, la modellazione dei primi neuroni artificiali (assieme alla successiva estensione a più strati), l'utilizzo del calcolo differenziale per l'aggiornamento dei pesi ed infine la formulazione del teorema di approssimazione universale sono sicuramente gli elementi fondamentali di questo successo. Alla fine degli anni '50 è stato modellato il primo neurone artificiale, prendendo ispirazione dal neurone biologico, composto dalla combinazione lineare di input e pesi in ingresso ad una funzione di attivazione.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/perceptron.png}
    \caption{Modello del neurone artificiale sulla base del funzionamento di un neurone biologico.}
    \label{fig:perceptron}
\end{figure}

\noindent Questo semplice meccanismo era in grado di mimarne grossolanamente il comportamento, generando una risposta a dei dati in ingresso, in modo tale che, superata una certa soglia, producesse o meno un valore in uscita. La funzione di attivazione era una semplice funzione gradino (al tempo non era scontato generare funzioni non lineari e continue), ma comunque questo oggetto era in grado di risolvere problemi di classificazione binaria. Il limite principale di questo modello consisteva nell'aggiornamento dei pesi in caso di predizioni sbagliate, basato su una delta di valori discreti. L'addestramento forniva in maniera euristica una direzione verso l'insieme ottimale delle variabili interne al modello, per ottenere la predizione il più possibile corretta ad ogni nuovo input. 

\noindent Per risolvere questo limite, venne definita una funzione di attivazione continua, trasformando il problema da uno di classificazione ad uno di regressione. Questa nuova costruzione permetteva di introdurre una funzione di perdita, nell'ottica di minimizzare l'errore nelle predizioni attraverso un approccio più rigoroso. Dalla teoria delle regressioni lineari infatti si poteva utilizzare il metodo dei minimi quadrati, che in termini pratici significava ridurre il più possibile l'errore nella rappresentazione della funzione che si voleva apprendere dai dati. Fino alla fine degli anni '60 si sperimentò l'utilizzo di questi modelli, di cui gli esempi più famosi sono Adaline e Madaline, costituiti da semplici reti di neuroni artificiali, rispettivamente ad uno e due strati. Essi non riuscivano a rappresentare correttamente le non linearità all'interno della distribuzione dei dati, ma si trattava solo di un limite tecnico e non teorico, poiché non erano ancora state introdotte funzioni di attivazione non lineari continue come la sigmoide e non era ancora stato compreso come propagare l'aggiornamento dei pesi negli strati nascosti. 

\noindent Nella seconda ondata di ricerca sulle reti neurali, iniziata negli '80, è stato dimostrato che è teoricamente possibile approssimare qualsiasi distribuzione dei dati attraverso l'apprendimento automatico di reti neurali con almeno uno strato di neuroni artificiali, aventi delle funzioni di attivazione non lineari. Questo teorema prende il nome di teorema di approssimazione universale. Esso si applica a tutte le tipologie più comuni di problemi risolti nel machine learning, quindi problemi discriminativi come la classificazione e la regressione e problemi generativi, come ad esempio l'encoding di immagini, la generazione di testo etc. Le implicazioni di questa dimostrazione hanno avuto un forte impatto solo in tempi più recenti, ma per comprenderne appieno le cause bisogna ancora revisionare alcuni elementi fondamentali in questa storia. Dalla neuroscienza infatti, non si è soltanto preso ispirazione per la modellazione del perceptron, tant'è che a partire dagli anni '50 è stato studiato il funzionamento della corteccia visiva nel cervello di alcuni mammiferi. Fondamentalmente con questi studi è stato dimostrato che i neuroni all'interno di questa zona sono organizzati gerarchicamente e nel livello più semplice rispondono a stimoli visivi con caratteristiche specifiche, come l'orientamento e le traslazioni. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/Hubel_and_Wiesel_cat.png}
    \caption{Gatto di Hubel e Wiesel\cite{a5catexp}. Studi che hanno permesso di dare una definizione di recettore, scoprire l'organizzazione gerarchica della corteccia visiva nei mammiferi e formalizzare il concetto di retinotopia.}
    \label{fig:chatto}
\end{figure}


\noindent Nel 1980 venne proposto il Neocognitron, un antenato delle moderne reti convoluzionali. In questo modello sono stati trasposti i precedenti principi, in quanto, oltre ad implementare una architettura gerarchica con strati di neuroni, è stato definito matematicamente come modellare dei campi recettivi, cioè in che modo identificare delle forme semplici con diverse orientazioni dall'immagine di input, come succede per i recettori delle immagini provenienti dal campo visivo oculare. La definizione è stata presa della teoria dei segnali usando la formula della convoluzione: 

\begin{equation} \label{eq:convolution}
y[n] = (x * h)[n] = \sum_{k=-\infty}^{+\infty} x[k] \cdot h[n - k]
\end{equation}

\noindent Classicamente, questa espressione permette la generazione di diversi filtri, in modo tale da modulare o isolare solo parti del segnale di interesse, eliminandone altre che possono non essere utili a successive trasformazioni o semplicemente perché fonti di rumore. Nel dominio dell'image processing si voleva sfruttare esattamente questa proprietà: applicare la funzione di convoluzione in modo da isolare le caratteristiche desiderate all'interno di una figura. Applicando questa formula nel dominio spaziale e definendo dei filtri bidimensionali, l'espressione assume la seguente forma: 

\begin{equation} \label{eq:convolution2D}
y[i,j] = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} h[m,n] \cdot x[i - m, j - n]
\end{equation}


\noindent Così non solo era possibile emulare il comportamento dei recettori visivi, ma allo stesso tempo si implementava il concetto di retinotopia. Il prodotto scalare di un filtro in una singola sezione dell'immagine, genera la stessa formula di un neurone artificiale, per cui ogni attivazione all'interno di ciascuna feature map (il risultato di una intera convoluzione) simula esattamente il modello del perceptron. La retinotopia definisce una relazione locale tra elementi vicini del campo visivo e neuroni vicini all'interno della corteccia visiva. Allo stesso modo attivazioni limitrofe nella feature map corrispondono ad elaborazioni di elementi prossimi nell'immagine. Nel complesso quindi, l'insieme di filtri e convoluzione permettono rispettivamente la rilevazione delle forme e la preservazione delle relazioni spaziali in un frame. Per ottenere invece un effetto di invarianza dalla posizione delle forme nell'immagine - in altri termini l'identificazione di queste indipendentemente da traslazioni - sono stati definiti degli strati di pooling. Questo modello presentava principalmente un grosso limite: il metodo di allenamento non era supervisionato e non si basava su una funzione di perdita globale, infatti l'utilizzo della backpropagation non era ancora stato formalizzato. Gli strati più interni della rete non permettevano la rappresentazione di forme più complesse e più coerenti con l'oggetto da classificare, come invece succede nelle reti moderne. Inoltre il Neocognitron era addestrato per il pattern recognition, ma non aveva una utilità pratica rispetto ai problemi più comuni nella computer vision. L'introduzione di uno strato fully connected, l'utilizzo di una funzione di perdita globale per la classificazione e della backpropagation portarono all'architettura di Lenet, nel 1998. L'allenamento di questa rete era specifico per la classificazione e tutti i neuroni dell'architettura partecipavano al training, quindi anche quelli degli strati convoluzionali. In altre parole si trattava di una rete end-to-end. AlexNet, la rete che segna una netta linea di demarcazione nella storia del deep learning, mantiene la stessa architettura, con una principale differenza: le funzioni di attivazione all'interno della rete permettono la propagazione del gradiente senza saturazioni negli strati più interni. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/Lenet.png}
    \caption{Lenet-5(1998). Primo modello ad aver dimostrato l'efficacia delle reti convoluzionali (CNN) nella comprensione delle immagini e ha aperto la strada a molte delle architetture moderne di deep learning. Crediti: Fei-Fei Li, CS231(Stanford).}
    \label{fig:Lenet-5}
\end{figure}

\noindent La riduzione dell'errore nei problemi di classificazione nelle applicazioni di visione artificiale è stata solo una naturale conseguenza: l'architettura ormai era chiara e funzionante, si trattava solo di aumentare il numero di neuroni e strati all'interno della rete, grazie ad una potenza di calcolo che ai tempi di Lenet non era disponibile.  
   

% In questa sezione verrà presentato il cloud computing come modello di erogazione di servizi IT. Si evidenzieranno i vantaggi in termini di scalabilità, flessibilità e riduzione dei costi infrastrutturali. Si collegherà il concetto all'Industria 4.0, spiegando come il cloud sia un elemento chiave per lo sviluppo di fabbriche intelligenti e connesse.
\section{Cloud Computing nell'Industria}

Il cloud computing rappresenta una delle innovazioni più rilevanti degli ultimi decenni nel settore IT, trasformando il modo in cui le aziende gestiscono le proprie risorse informatiche e processi produttivi. Questo nuovo paradigma, basato sull’erogazione di servizi tramite Internet, consente di accedere a risorse come server, storage, database e applicazioni software in modo scalabile e on-demand, senza dover effettuare investimenti iniziali significativi in infrastrutture hardware. Le implicazioni di questa trasformazione sono profonde, in quanto ridefiniscono i modelli di gestione IT e le strategie aziendali, favorendo un approccio più veloce e flessibile nell’implementazione di nuove soluzioni. La principale innovazione apportata dal cloud computing risiede nella possibilità di adattare rapidamente le risorse informatiche alle necessità aziendali, garantendo una scalabilità notevolmente superiore rispetto alle infrastrutture tradizionali. In passato, le aziende che desideravano espandere i propri sistemi erano costrette a effettuare investimenti consistenti in hardware e a sostenere costi elevati per la relativa gestione e manutenzione. Inoltre, la diversa geolocalizzazione dei datacenter comporta vantaggi in termini di accessibilità, permette di risolvere problemi di latenza e di personalizzare i servizi in base alla regione in cui l'applicazione eseguita sul cloud viene deployata. 


Oltre a facilitare la gestione delle risorse e ridurre i costi infrastrutturali, il cloud computing è considerato una tecnologia abilitante nell'implementazione dell’Industria 4.0. Ogni era industriale è stata segnata da una svolta tecnologica: nella prima l'introduzione della macchina a vapore, nella seconda l'elettrificazione delle macchine e la conseguente implementazione della catena di montaggio. La terza rivoluzione è stata possibile grazie all'invenzione del transistor e la successiva democratizzazione dei calcolatori. Questa nuova ondata invece è incentrata sui dati: ad esempio, nel 2015 è stato stimato che solo l'1\% delle informazioni generate dai sensori all'interno di una fabbrica veniva effettivamente elaborata. Il trend tuttavia è destinato a cambiare, con un valore associato alle informazioni estratte dal mondo reale nell'ordine di $10^3$ miliardi di dollari entro il 2025 \cite{a6mckinsey}.  L’adozione di tecnologie quali l’Internet of Things (IoT), gli sviluppi moderni nell'intelligenza artificiale e l’analisi di big data sono gli elementi che concorrono a questa nuova rivoluzione. Il primo di questi fattori è fondamentale per la generazione e l'ingestion, mentre gli altri due per il processamento: indipendentemente dalle loro funzioni, i dati restano il fulcro delle operazioni. In questo contesto, il cloud fornisce l'infrastruttura e i servizi necessari per l'integrazione di questi elementi, rendendo possibile l'implementazione delle smart factories. 

La capacità del cloud di raccogliere, archiviare ed elaborare grandi quantità di dati in tempo reale è cruciale per sfruttare appieno il potenziale dell’Industria 4.0. Le aziende che operano in settori industriali tradizionali, come la manifattura, possono trasformare le loro linee di produzione in sistemi autonomi e ottimizzati, capaci di adattarsi alle esigenze del mercato e di ridurre significativamente gli sprechi. La connettività fornita dal cloud consente invece di collegare dispositivi, sensori e macchinari all'interno della fabbrica, creando un ecosistema in cui ogni componente è in grado di comunicare e condividere le proprie informazioni, rendendo più semplice il monitoraggio dei processi produttivi. Inoltre con gli avanzamenti nella ricerca sul deep learning, diventato sempre più consistente negli anni, i relativi modelli sono stati adottati per migliorare diverse parti dell'ecosistema aziendale, come il monitoraggio, i processi decisionali e produttivi. Un esempio concreto è la manutenzione predittiva, che sfrutta i dati provenienti dai sensori per rilevare anomalie e prevedere i guasti delle macchine. E' così possibile ridurre i tempi di inattività, prolungare la vita utile delle apparecchiature e migliorare la loro efficienza complessiva. Sempre nello stesso contesto, un'azienda potrebbe utilizzare il cloud per raccogliere e analizzare dati provenienti dalle linee di assemblaggio, applicando modelli di apprendimento automatico per migliorare la qualità dei componenti e ridurre i difetti di produzione. La sicurezza in fabbrica, oggetto di questa trattazione, rientra chiaramente nel dominio delle applicazioni in questa fase industriale. 

\section{Lavori Correlati}

In letteratura, sono stati individuati due approcci che rientrano nell'ambito di questa ricerca. Nel complesso, dimostrano delle differenze nella modalità di implementazione del sistema, rispetto a quanto realizzato in questa tesi, ma aiutano a fornire contesto alla trattazione. Il primo metodo \cite{a7edgeppe} è totalmente integrato con il cloud, ma si serve del provider Microsoft Azure. L'obiettivo del paper è quello di analizzare direttamente su una telecamera IoT, in real time, l'input proveniente da una Learning Factory, un ambiente che simula quello di un impianto manufatturiero. Si tratta di un luogo pensato a scopo di training e di ricerca, ma non per questo esente da rischi. Il modello eseguito sul SoC del dispositivo è stato allenato e customizzato per essere lanciato su dell'hardware con una potenza di calcolo limitata. La fase di generazione del modello avviene sul cloud, successivamente, una versione leggera viene caricata sul dispositivo. Come nel sistema implementato in questa tesi, l'architettura è di tipo edge-cloud. Le differenze sostanziali consistono nella locazione del modello e nella modalità in cui esso viene acceduto. Non c'è bisogno infatti di invocazioni a servizi nel cloud una volta che l'algoritmo di machine learning è stato deployato sull'edge, rispetto a quanto succede invece nel sistema proposto. In particolare, l'implementazione oggetto di questa tesi, chiama una API per un modello già specializzato nell'analisi dei dispositivi di sicurezza, come si vedrà nel capitolo successivo. Nel related work, inoltre, l'obiettivo è l'identificazione degli occhiali protettivi, mentre il modello utilizzato in questa soluzione è in grado di rilevare ulteriori classi di dispositivi di sicurezza. L'algoritmo deployato sull'edge raggiunge l'obiettivo di inferenza in tempo reale, ma le prestazioni sono comunque limitate, indipendentemente dal tipo di allenamento eseguito. A questo proposito, sono stati seguiti due approcci: nel primo caso il training è effettuato su immagini presenti in rete, di fatto poco adattabile allo scenario della Learning Factory. Nel secondo invece, sono state utilizzate immagini provenienti dal sito, ottenendo un benchmark migliore, seppur limitato.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/relw1.png}
    \caption{Prestazioni Learning Factory: custom dataset.}
    \label{fig:relw1}
\end{figure}


%Quindi difficilmente una customizzazione del modello offerto da Amazon può ottenere delle prestazioni migliori.
Il modello rileva tre classi principali: occhiali protettivi, volti e persone. Gli ultimi due tag sono importanti per capire se i dispositivi sono indossati, mentre il primo serve per l'identificazione dell'oggetto di interesse. I risultati confermano la difficoltà generale dei modelli nel trovare elementi piccoli all'interno di un frame. Sul totale dei casi in cui il modello è in grado di identificare un oggetto, solo la metà sono occhiali protettivi. In generale l'algoritmo riesce ad identificare pochissime volte questo tipo di dispositivo, proprio per le loro piccole dimensioni.

Il secondo approccio analizzato \cite{a8safety4}, propone una soluzione completamente on premise. Si tratta di un meccanismo molto più sofisticato rispetto all'oggetto di questo scritto, ma, trattandosi di un lavoro recente e con prestazioni ottimali, permette di ottenere una panoramica migliore sugli sviluppi in questo campo. Il sistema proposto non si limita solo a rilevare i dispositivi di sicurezza, ma anche gli incidenti e il controllo delle macchine. In base a ciò che il modello utilizzato riesce ad identificare, viene generata una matrice composta dalla probabilità che un evento dannoso possa verificarsi e dalle classi di gravità dell'infortunio. Il sistema, al matching sulla matrice, deve rispondere opportunamente. Questo meccanismo introduce un controllo della granularità nella reazione agli eventi, in quanto, ad esempio non ha senso spegnere un macchinario se un lavoratore non ha un dispositivo di sicurezza indossato, ma allo stesso non si trova in un'area pericolosa. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/safety-system.png}
    \caption{Sistema per la gestione della sicurezza in fabbrica.}
    \label{fig:safety-system}
\end{figure}

La ricerca si basa sempre su modelli di apprendimento automatico per la computer vision: si tratta di una soluzione sull'edge, ma non a livello di telecamere. Non c'è una integrazione con il cloud, infatti l'algoritmo, una Faster R-CNN, viene addestrato in Colab e successivamente deployato su un server all'interno della fabbrica. Questa tipologia di rete neurale convoluzionale è un ottimo trade-off in termini di utilizzo delle risorse, di accuratezza nelle rilevazioni e di velocità di esecuzione. Vengono mostrate le metriche su diverse Intersection of Union (IoU) per le classi di DPI in esame, cioè casco protettivo e giubbotti di sicurezza. Anche in questo studio è stato utilizzato un dataset ad hoc per migliorare l'accuratezza nella rilevazione. I risultati ottenuti, raggiungono lo stato dell'arte per il dominio di applicazione, con una mean average precision di 0,74 e una mean average recall dello 0,83.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/safety-system-results.png}
    \caption{Risultati del sistema di rilevazione.}
    \label{fig:safety-system-res}
\end{figure}  