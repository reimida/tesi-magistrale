\chapter{Implementazione del sistema}

\section{Architettura}
%prova a definire gli use case con una tabella, in generale espandi questa parte.
L’architettura proposta è di tipo event-driven, per cui il processamento dei dati avviene solo quando si verifica una situazione particolare sull’edge, dato il seguente scenario: uno o più operatori entrano all’interno di una certa area definita da un insieme di ancore, su cui sono montati dei sensori e si trovano in prossimità di un macchinario attivo; due telecamere, una superiore ed una frontale monitorano l’area di sicurezza. Se almeno uno degli operatori non possiede i dispositivi di sicurezza, il sistema deve generare un allarme e spegnere il macchinario. Lo stesso tipo di reazione deve verificarsi se almeno uno degli operatori non è abilitato ad operare sulla macchina. Infatti, coloro che possono lavorare indossano un tag attivo che trasmette informazioni ai sensori sulle ancore.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/use-case.png}
    \caption{Schema dello use case.} 
    \label{use-case}
\end{figure}


 Il design è quello di un’ applicazione cloud nativa e si divide in due sottosistemi tra di loro connessi: da un lato l'edge, composto da una telecamera superiore accoppiata ad una frontare, dei sensori ed un gateway; dall'altro il cloud formato da servizi gestiti dal provider AWS. Il progetto non è una classica applicazione client-server, ma un insieme di servizi gestiti che globalmente opera near-real time e si serve di dispositivi IoT sia per la rilevazione degli eventi che per l’ingestion dei dati sul cloud. Nel primo caso viene utilizzato MQTT per la telemetria. Nel secondo invece, vengono connesse le telecamere ad un gateway attraverso il protocollo RTSP e successivamente questo si occupa dell'ingestion effettiva, sfruttando una libreria Amazon apposita. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/architettura.png}
    \caption{Architettura ad alto livello del sistema.} 
    \label{fig:architettura}
\end{figure}
 

Il gateway posto tra i dispositivi e il cloud non serve solo per garantire la necessaria potenza di calcolo per l’invio dei dati, ma è un componente imprescindibile per l’integrazione con AWS. Al suo interno infatti è installato un software fondamentale per la realizzazione del sistema: IoT Greengrass. Una delle sue funzionalità più importanti è quella di installare sull'edge componenti software caricati dal cloud. Esso consente uno sviluppo del progetto molto più rapido e semplice, consen. Si occupa di problemi come: 


\begin{itemize}
	\item \textbf{sicurezza sull’edge e dei dati in transito}: Greengrass è installato con permessi di root e sfrutta le user permission per l’anti-tampering del codice dopo il deploy. Per la comunicazione in entrata ed uscita, si serve di certificati relativi alla public key infrastructure (PKI) rilasciati da aws. Per cui, tramite autenticazione, cifratura e integrità dei dati sia la comunicazione che i deploy sono sicuri di default.
	\item \textbf{orchestrazione del runtime}: i componenti che vengono deployati dal cloud verso l’edge, in qualsiasi forma essi siano (monoliti, servizi o container) possono essere gestiti dall’ambiente in modo da definire dipendenze tra i vari componenti ed un lifecycle per ciascuno di essi.  
	\item \textbf{logging e monitoraggio}: questa integrazione è fondamentale soprattutto per dispositivi che non sono facilmente accessibili da remoto. I log generati dai componenti possono essere automaticamente sincronizzati con servizi cloud aws come cloudwatch. Inoltre lo stato dei dispositivi è automaticamente tracciato in modo tale da controllare quelli che riscontrano dei problemi e rispondere rapidamente agli errori.
	\item \textbf{scalabilità del deployment}: con Greengrass possono essere definiti gruppi di dispositivi su cui deployare dei componenti. Un dispositivo può appartenere anche a più gruppi e può quindi ricevere diversi deployment contemporaneamente. La definizione di questi insiemi permette quindi un controllo granulare delle istanze per le quali serve fare merging di diversi deployment.
\end{itemize}

L'obiettivo di questi primi paragrafi è quello di descrivere i componenti hardware e software sull’edge ed i servizi gestiti sul cloud che si occupano dell’ingestion dei dati in maniera sicura, robusta e scalabile, con accenni sulla trasformazione e il preprocessamento dei dati(vedi Figura \ref{fig:sub-ing}). In questa soluzione, Greengrass è installato all’interno di un container Docker che ha come immagine di base la più recente versione di Amazon Machine Image (AMI), pensata per essere integrata con AWS e quindi essere più efficiente e sicura all’interno dell’infrastruttura cloud. Questa macchina virtuale è stata customizzata in modo da supportare sia le dipendenze di Greengrass, che quelle per GStreamer, un tool per l’inoltro dello streaming, che collabora con la libreria apposita per l'invio sul cloud. L’ingestion dei dati video e di quelli provenienti dai sensori è separato ed avviene in due modalità differenti. Una volta che il componente video viene deployato inizia ininterrottamente a streammare su KVS, mentre il publisher che genera l’evento, inviandolo ad un broker su AWS IoT, si attiva solamente alla rilevazione da parte dei sensori di uno o più operatori. 

Attraverso Greengrass è possibile generare lo stesso runtime che sul cloud permette l’esecuzione di uno dei componenti più noti in AWS, cioè Lambda, una funzione gestita direttamente dal provider, che elimina la necessità di configurazione di macchine virtuali, sistemi operativi e scalabilità. Il tutto avviene nell’ottica di concentrarsi solo sullo sviluppo della logica per la reazione agli eventi. In questo modo i sensori pubblicheranno i dati su una coda MQTT locale, come il numero di tag rilevati o la macchina cui si fa riferimento, e successivamente la Lambda sull'edge, iscritta al relativo topic, si attiverà alla pubblicazione di queste informazioni. Una volta eseguita una prima elaborazione, la funzione pubblicherà su un topic in IoT Core, generando un evento in formato JSON. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/sottosistema-ingestion.png}
    \caption{Sottosistema di ingestion e preprocessing.} 
    \label{fig:sub-ing}
\end{figure}

Al messaggio MQTT vengono applicate delle regole, in modo tale da filtrare i dati, successivamente trasmessi ad un’altra Lambda che si trova sul cloud, la quale si occupa del processamento dello streaming video. A valle dell’analisi viene prodotto uno stream di dati verso un’applicazione big data. Nel dettaglio, il flusso iniziale viene separato in due, in quanto la Lambda è progettata per elaborare uno streaming video alla volta ed i flussi streaming da analizzare in parallelo provengono, come già visto, da una telecamera frontale e una superiore. Per cui ogni evento che triggera la Lambda deve contenere un riferimento allo stream: in questo modo, contemporaneamente, due istanze diverse della stessa Lambda verranno attivate a valle del topic filter. 

Per concludere il quadro della descrizione architetturale, bisogna focalizzarsi sui dati in uscita dalla Lambda. Ogni frame analizzato dallo streaming video viene inviato in modalità asincrona sul servizio gestito Kinesis Data Stream (KDS), il quale permette la trasmissione di grandi flussi di dati in real-time. Queste informazioni possono essere successivamente analizzate in parallelo su dei cluster, dopo aver deployato una opportuna applicazione big data. Si tratta quindi del centro di elaborazione all'interno dell'architettura, in quanto è il punto in cui tutti gli eventi vengono uniti dalle diverse sorgenti IoT, che siano le telecamere o i sensori. In questo progetto è stata sviluppata una applicazione con Apache Flink, come già visto nel capitolo precedente, il framework e motore di processamento distribuito per flussi di dati streaming sia limitati che illimitati. 
%rivedi questa parte
A differenza di altre soluzioni, come Apache Spark, questo framework non solo supporta il batching, ma anche l’analisi di singoli record provenienti da un flusso di dati real-time. Amazon offre un servizio gestito per l’analisi chiamato Kinesis Data Analytics, ma per gli esempi e la ridotta scala dei dati è stato scelto di deployare l’applicazione su una istanza EC2. Il risultato è equivalente e il codice non deve essere modificato per funzionare su un eventuale cluster. Infine, all'estremo del sistema è presente un'ultima Lambda, la cui responsabilità è quella di restituire gli allarmi generati in fase di processamento. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/processing-subsystem.png}
    \caption{Moduli di processamento e generazione degli allarmi.} 
    \label{fig:processing-subsystem}
\end{figure}

\section{Edge Runtime e Ingestion}

La soluzione edge, ossia l'insieme dei componenti hardware e software che dall'impianto comunicano con il coud, segue una topologia a stella. Al centro è posizionato il gateway, il quale si interfaccia con AWS da un lato, e con i dispositivi dall'altro. Più nello specifico, si tratta di una soluzione ibrida, perché uno schema centralizzato dovrebbe essere autosufficiente, ossia la sua operatività non dipende dallo stato della rete. All'interno del gateway, è installato un Docker container, composto da una serie di strati. Al primo livello è installata l'ultima versione della AMI, formata da un ambiente minimale che si basa sulle distribuzioni Linux Fedora e CentOS. In questo sistema non è possibile installare qualsiasi tipo di libreria, ma solo un numero limitato, fornito da un repository remoto di AWS. Il tutto avviene nell'ottica di una integrazione ottimizzata con il cloud, ma soprattutto per ottenere una sicurezza migliore. Avere un sistema il più possibile ridotto e con un controllo capillare delle installazioni permette di ridurre la superficie di esposizione ad attacchi informatici. Nello strato successivo, vengono instalate delle configurazioni specifiche per Greengrass all'interno dell'ambiente containerizzato. Si sarebbe potuto integrare il runtime direttamente sull'host, ma questo richiede più tempo a causa dei problemi di compatibilità e configurazione. Generando invece un container apposito è possibile replicare questo sistema su tutte le macchine dell'impianto ed in generale distribuirlo a più clienti. Vengono successivamente installate le dipendenze per Greengrass, ad esempio una Java virtual Machine, fornita da Amazon, in modo tale che questo servizio possa essere effettivamente eseguito all'interno del Docker. Contestualmente, in questo strato, una volta che le dipendenze sono pronte, si passa all'installazione effettiva di Greengrass. Lo strato successivo del filesystem è composto dalle librerie per GStreamer, il tool su cui si basa lo stream producer per KVS. Queste dipendenze non sono referenziate dal package manager di AWS, ma indebolendo opportunamente i vincoli di sistema della AMI, è possibile scaricare la giusta versione delle librerie. La customizzazione non è semplice, in quanto gli intervalli di compatibilità tra le librerie del kernel e qualsiasi altra installazione sono molto stringenti. Ad esempio, la regola più forte che si osservata durante la customizzazione è stata sulla versione di SSL, forzata ad essere quella più recente, ma non supportata dalla maggior parte delle release di GStreamer, sia a livello di dipendenze che di libreria stessa. L'ultimo strato, infine, è composto dall'installazione effettiva di questo tool.







%gstreamer, breve panoramica e supporto ad rtsp e codifiche + definizione di pipeline
In generale, GStreamer viene usato per costruire una pipeline, ossia un grafo orientato composto da operatori per la gestione di dati multimediali. Il componente software, eseguito sul gateway, internamente sfrutta una libreria che accede a GStreamer. %Per semplicità nella trattazione, verrà mostrata la generazione della la pipeline per lo streaming video con il plugin gst. Si tratta di uno strumento che può essere lanciato direttamente dalla linea di comando, ed il funzionamento è identico a quello del software. 
Analogamente, anche le telecamere generano con lo stesso meccanismo i flussi RTSP. Nella configurazione, deve esserci anche un server che utilizza lo stesso protocollo, in modo tale da consentire l'inoltro dello streaming al gateway. L'ipotesi nello use case è di avere il setup già disponibile in fabbrica, con i due flussi RTSP già presenti. Poiché si sta cercando di simulare questo scenario, è stato installato mediamtx, un container di un server multimediale open-source per diversi tipi di streaming, tra cui RTSP. L'obiettivo dovrebbe essere quello di potersi agganciare agli stream ed inoltrarli in maniera sicura verso il cloud. Se le telecamere sono più sofisticate, AWS offre la possibilità di inviare i flussi direttamente sul cloud registrando i dipositivi in AWS IoT, ma in generale come meccanismo standard è utile convogliare le informazioni sul gateway per diverse motivazioni. In primo luogo, l'utilizzo di un intermediario permette una gestione dello streaming più sofisticato. Inoltre il gateway possiede risorse di calcolo superiori rispetto a delle telecamere IoT. Questo, ad oggi, è fondamentale nel caso in cui si voglia deployare dei modelli di machine learning direttamente sull'edge, naturale estensione di questo sistema.

Esistono diversi blocchi per la costruzione della pipeline, ma in questo caso, trattandosi di generazione ed inoltro di streaming, sono stati usati i seguenti elementi: source, sink, filtri e codec. Il source è un elemento che porta i dati dall'esterno verso la pipeline. Ad esempio si può accedere alla telecamera di un computer, file multimediali e soprattuto flussi provenienti dalla rete, basati su protocolli come RTSP ed HTTP. Il sink invece esegue l'operazione contraria: questo operatore può caricare l'output della pipeline in memoria, su dell'hardware esterno come un monitor, oppure inoltrare direttamente il flusso su un endpoint. I filtri e i codec servono per operazioni intermedie, nel primo caso per la modifica dello stream come il framerate o la dimensione del frame, mentre nel secondo per cambiare la codifica delle immagini. 


%\begin{lstlisting}[language=GStreamer, caption={Pipeline GStreamer per uno stream RTSP}, label={lst:gstreamer}]
%gst-launch-1.0 -v \
%v4l2src device=/dev/video2 ! \
%videoconvert ! \
%videoscale ! \
%video/x-raw,format=I420,width=1280,height=720,framerate=30/1 ! \
%x264enc key-int-max=5 tune=zerolatency ! \
%video/x-h264,stream-format=avc,alignment=au,profile=baseline ! \
%rtspclientsink location=rtsp://user:password@localhost:8554/top
%\end{lstlisting}

%per una analisi coerente delle pipeline aspetta che sonnet sia disponibile, hai una chat con gpt dove è salvato il corretto flusso di prompt.  
%mostra tutte le pipeline ed infine come viene usata la api nel componente greengrass
%mostra come vengnono pubblicati i messaggi mqtt sul cloud, nonostante non abbia l'infrastruttura
Per quanto riguarda la pubblicazione degli eventi, la Lambda, che si occupa del preprocessing, viene triggerata con un messaggio pubblicato su un topic, come si può notare dall'esempio sulla console AWS. Rispetto al flusso di dati non strutturato proveniente dalle telecamere, le informazioni dei sensori sono estrememante minimali, in quanto MQTT, essendo un protocollo leggero, è soprattutto pensato per questo genere di messaggi. L'evento è formato da campi per l'identificazione univoca della macchina associata ad una certa area di sicurezza, e allo stesso tempo referenzia le telecamere che monitorano il luogo di interesse. Si poteva anche generare due messaggi MQTT per identificate gli stream video separatamente, ma l'overhead è poco significativo in termini di spazio. Inoltre, in una configurazione con due messaggi per lo stesso evento sarebbe stato necessario replicare le informazioni relative alla macchina e all'area di sicurezza. I topic filter di AWS IoT permettono, alla generazione delle regole, di creare due trigger separati, verificando che sono presenti due stream diversi nel JSON, quindi la separazione degli eventi avviene direttamente nel cloud. Questo è soprattutto un vantaggio in termini di sincronizzazione, perché i messaggi non rischiano ritardi nella rete, e le due istanze della stessa Lambda verranno attivate ad intervalli praticamente sovrapposti. I messaggi, infatti, si trovano già nella stessa infrastruttura, e l'ordine di grandezza in termini di latenza si riduce notevolmente rispetto a quello della rete pubblica. %Ciò avviene sotto l'ipotesi che il cloud non sia sovraccarico, se succedesse si possono trovare meccanisimi di reazione a questo scenario, che comunque è in gran parte limitato.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/publish-example.png}
    \caption{Pubblicazione messaggio MQTT sul servizio AWS IoT.} 
    \label{fig:IoT-publish}
\end{figure}

 
\section{Preprocessing e trasformazione dei dati}
La parte di preprocessing, come visto nella descrizione generale, avviene tramite il coordinamento di diversi managed services, il cui fulcro è la funzione Lambda. Essa implementa la trasformazione dello streaming video in uno flusso di dati in tempo reale, integrando vari servizi AWS in maniera scalabile, efficiente e soprattuto automatica, in base al numero di stream ricevuti. I servizi utilizzati all'interno della Lambda sono Kinesis Video Streams (KVS), Amazon Rekognition, Kinesis Data Streams ed opzionalmente S3. La funzione è implementata in Python e sfrutta internamente il multithreading per l'ottimizzazione delle prestazioni. Alla base di questo sistema c'è il \texttt{lambda\textunderscore handler}, i.e. l'entryponit del codice, invocato all’arrivo di un nuovo evento. In termini più specifici, questo tipo di attivazione prende il nome di trigger e viene generato attraverso una topic rule, un meccanismo di filtro fornito da IoT Core. Poiché si tratta del payload di un messaggio MQTT, l'evento viene inviato sotto forma di JSON. Il filtro esegue quindi due operazioni: in primo luogo verifica che il topic sia quello di interesse della Lambda; successivamente può modificare il contenuto del messaggio, scartando alcuni campi, in base a come è stata definita la regola sul servizio. In generale, nella maggior parte dei casi, la comunicazione dei componenti all'interno del cloud è completamente trasparente all'utente e molti servizi non necessitano dell'invocazione diretta delle API, come invece capita normalmente nelle architetture a microservizi. La gestione viene delegata internamente all'infrastruttura AWS. Nelle parti in cui invece l'utente deve scrivere codice, la situazione è diversa: AWS in questo caso fornisce gli SDK in base al linguaggio di programmazione scelto. Nell'implementazione della funzione Lambda si sfrutta la libreria botocore per invocare le API di Rekognition e KDS.

L'algoritmo della Lambda, chiamata Kinesis Video Consumer, inizialmente si occupa di ricevere l'evento, contente il nome del flusso video da processare. Questo dato viene estratto e validato: se mancante, la funzione registra un errore e termina prematuramente. Successivamente viene stabilita una connessione con KVS, per ottenere l'endpoint da cui scaricare il flusso video. A questo punto viene creato un client apposito per accedere ai dati tramite l'API di KVS \texttt{get\textunderscore media}, agganciandosi allo stream da un momento specifico, definito tra le opzioni della chiamata. Poiché l'obiettivo del sistema è quello di elaborare un flusso in tempo reale, questo parametro deve essere impostato sul frame corrente, cioè l'ultimo che è stato caricato nel servizio KVS. Dopo il collegamento all'endpoint e l'inizializzazione del flusso, inizia l'elaborazione del video vera e propria. Lo stream viene processato iterativamente, elaborando i frammenti ricevuti dal KVS. %vedi se riesci a spiegare la logica di frammentazione.
Questa unità contiene al suo interno un certo numero di frames, il cui valore è configurabile dalla sorgente del flusso multimediale %vedi se riesci a parlare della codifica. "e.g. e come si vedrà più o avanti o come si è visto prima questo valore può impattare sulla latenza"
I relativi bytes vengono progressivamente accumulati in un buffer temporaneo, per raggiungere il confine di separazione tra un pezzo di video ed un altro. Una volta che il fragment è stato ricostruito, vengono estratti i frames contenuti al suo interno. Essi vengono successivamente codificati in un formato accettabile per la API di Rekognition. 


A questo punto ciascun frame viene inviato al servizio per l'analisi delle immagini, in particolare si sfrutta \sloppy\texttt{detect\textunderscore protective\textunderscore equipment}\fussy, specificando il tipo di DPI da rilevare ed una soglia di confidenza minima. Le chiamate avvengono in parallelo sfruttando il thread pooling, con la relativa struttura dati inizializzata prima dell'esecuzione dell'handler. Le variabili globali definite all'interno della Lambda infatti, permettono di conservare lo stato delle informazioni ad ogni nuova esecuzione della funzione, quindi al verificarsi di un nuovo evento. Quando possibile, è estremamente utile usare questo meccanismo in modo tale da ridurre al massimo il tempo di elaborazione. Inoltre, sempre in termini di efficienza, occorre che i trigger si attivino almeno ogni 15 minuti, in modo tale che non ci sia una deallocazione delle risorse associate alla Lambda. La frequenza degli eventi è imprevedibile, per cui ha senso programmarli al superamento di una certa soglia temporale. E' possibile implementare questa logica attraverso i timer impostati con il servizio CloudWatch. Infine, sempre in maniera asincrona, il JSON risultante dall'analisi viene inviato al servizio gestito KDS, invocando una opportuna API.




\section{Big data analysis}
L’applicazione big data è formata da classi per la modellazione dei dati in ingresso, per la rappresentazione dei dati trasformati e per la effettiva elaborazione. % Il main è contenuto all’interno di SafetyOnPlantsStreamingJob.java (//TODO: spiega perché vengono definiti jobs all’interno di un cluster; in generale dovresti fornire dettagli su come funziona il framework flink).

Il primo gruppo di classi, associato al data model, è fondamentale per la rappresentazione dei dati streaming in ingresso all’applicazione, provenienti dall’analisi del video. Il tutto è strutturato in modo da essere modulare e facilmente manutenibile sfruttando il paradigma della programmazione ad oggetti. La composizione delle classi permette la deserializzazione in runtime del JSON proveniente dall’elaborazione di Amazon Rekognition. %(il servizio gestito che contiene i modelli per la object detection), strutturato nel seguente modo:
Esse sono strutturate nel seguente modo:

\begin{itemize}
	\item \textbf{RekoResult}: È la classe principale che rappresenta il risultato di un'analisi video per la rilevazione dei dispositivi di protezione individuale. Contiene la versione del modello machine learning che ha analizzato i frame durante la fase di preprocessing, il numero totale di frame analizzati, un riepilogo dell'equipaggiamento rilevato, informazioni sulla dispositivo sorgente ed una lista di persone rilevate.
	\item \textbf{Summary}: Fornisce un riepilogo degli individui rilevati nel video, suddividendo gli identificativi in tre categorie: persone con l'equipaggiamento richiesto, senza l'equipaggiamento richiesto e con equipaggiamento indeterminato. In quest'ultimo caso il modello non è stato in grado di distinguere se un lavoratore indossasse correttamente i dispositivi.
	\item \textbf{SourceInfo}: Questa classe contiene informazioni dettagliate sulla fonte del video, quindi le informazioni sulla telecamera, l'indetificativo del sito industriale, quello del gateway e della macchina cui si sta facendo riferimento.
	\item \textbf{Camera}: Modella i dettagli della telecamera utilizzata, come l'identificativo, il tipo, lo stream video, e la telecamera associata.
	\item \textbf{Person}: Rappresenta una persona rilevata, includendo informazioni sulla posizione nel video, un elenco di parti del corpo rilevate, la confidenza del modello nella rilevazione della persona, e un idnetificativo univoco per la persona stessa.
	\item \textbf{BodyPart}: Descrive una parte del corpo di una persona rilevata, associando una confidenza nella rilevazione, e una lista di dispositivi di protezione rilevati su quella parte del corpo.
	\item \textbf{EquipmentDetection}: Rappresenta un dispositivo di protezione rilevato su una parte del corpo, con dettagli sulla posizione, il tipo di equipaggiamento, la confidenza della rilevazione, e se copre adeguatamente la parte del corpo in questione.
	\item \textbf{CoversBodyPart}: Questa classe modella il risultato dell'analisi di copertura di una parte del corpo attraverso il dispositivo di protezione. Include un valore booleano che indica la copertura e un valore di confidenza per tale rilevazione.
	\item \textbf{BoundingBox}: Utilizzata sia per le persone che per i dispositivi di protezione, questa classe definisce le coordinate di un'area rettangolare in un'immagine o video.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/data-model.png}
    \caption{Diagramma modello dati.} 
    \label{fig:data-model}
\end{figure}

Le classi di processamento e trasformazione sono associate invece al Job, che come visto nel capitolo precedente, rappresenta il nucleo del sistema di elaborazione. Il dataflow graph logico, generato dall'applicazione, si occupa di processare flussi di dati provenienti da Kinesis Data Stream, i.e. il source. L'analisi in questa prima fase avviene su due flussi differenti, poiché  dipende dal tipo di telecamera che ha generato gli stream video. Quella superiore infatti conta il numero di persone nell'area, mentre quella frontale rileva i dispositivi di sicurezza. Una volta connesso alla sorgente, si verifica la lettura dei dati in ingresso, rappresentati in JSON. Successivamente si applica un filtro, in modo tale da operare solo sulle informazioni relative all'analisi delle telecamere, poiché l'applicazione, in chiave di estensione del sistema, potrebbe ricevere altre tipologie di elementi dallo stesso Kinesis Data Stream. A valle del filtro, un operatore di mapping si occupa della deserializzazione dei dati, il che semplifica l'elaborazione successiva, potendo invocare dei metodi sopra gli oggetti generati a partire dal data model. Un secondo filtro viene invoca in modo tale poter discriminare i flussi delle due telecamere. Si otterebbero altrimenti dei duplicati senza questa distinzione. L'operatore keyby si occupa del loro raggrupamento in base al dispositivo, poiché si vuole ottenere in modo coerente l'analisi di un campione di frame. La logica infatti consiste nell'avere una percentuale di immagini, in cui l'accuratezza della rilevazione o del conteggio superi una certa soglia. Ogni telecamera IoT, in un breve intervallo temporale produrrà un certo numero di frame, per cui il raggruppamento serve per poter distinguere quelli prodotti da ciascuna di esse. Operativamente, l' aggregazione in modalità streaming viene effettuata con le finestre di conteggio. In maniera semplice si poteva utilizzare direttamente la funzione window, fornita dalla API Datastream, ma si è preferito optare per una implementazione di più basso livello. In questo modo si è potuto maneggiare direttamente lo stato ed osservare la logica di funzionamento per l'implementazione della process function nella coda della catena di operazioni seguenti. L'operazione FlatMap permette di poter maneggiare lo stato e le finestre a più basso livello, anche se nativamente non è un operatore strettamente stateful. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/system-logical-dataflow-graph.png}
    \caption{Dataflow logico del sistema.} 
    \label{fig:system-dataflow}
\end{figure}

I due datastream vengono poi uniti, creando un unico flusso attraverso l'applicazione di una finestra temporale. Questa unione permette di correlare i dati provenienti dalle diverse tipologie di telecamere e di sincronizzare i due stream. Per evitare duplicati, un possibile effetto collaterale della finestra scorrevole, i dati vengono ulteriormente filtrati per assicurarsi che ogni richiesta proveniente dall'edge venga processata una sola volta. La fase finale del processamento prevede l'applicazione di regole specifiche per generare gli allarmi. Questi vengono prodotti solo quando la macchina monitorata è attiva e quando le condizioni di sicurezza, definite come una percentuale minima di rilevamento dell'equipaggiamento protettivo e di discrepanza nel conteggio delle persone tra sensori e modello di visione artificiale, non sono soddisfatte. Gli allarmi così generati sono rappresentati come coppie di valori, e vengono passati a una funzione Lambda per l'elaborazione finale. In particolare, la struttura dati identifica la richiesta originale inviata dall'edge tramite messaggio MQTT e il timestamp del momento di generazione dell'allarme.



\section{Deployment}